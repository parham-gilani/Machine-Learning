{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Phase2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Studnet number : 400101107 - 400101859\n",
    "\n",
    "Student name : Sadra Khanjari - Parham Gilani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Question Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler,Binarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit as sigmoid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first part of the simulation question. This is our Secure Restricted Boltzmann Machine (SRBM) implementation and as you can see it has a sample hidden layer and a sample visible layer. The visible layer is the input layer and the hidden layer is the output and it also has a train method that trains the model. There is a also a transform method that transforms the input to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecureRBM:\n",
    "    def __init__(self, n_visible: int, n_hidden: int):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.W = np.random.randn(n_visible, n_hidden) * 0.01\n",
    "        self.b = np.zeros(n_visible) \n",
    "        self.c = np.zeros(n_hidden)  \n",
    "\n",
    "    def sample_hidden(self, visible: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sample hidden units given visible units\"\"\"\n",
    "        activation = np.dot(visible, self.W) + self.c\n",
    "        probabilities = sigmoid(activation)\n",
    "        return np.random.binomial(1, probabilities)\n",
    "\n",
    "    def sample_visible(self, hidden: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sample visible units given hidden units\"\"\"\n",
    "        activation = np.dot(hidden, self.W.T) + self.b\n",
    "        probabilities = sigmoid(activation)\n",
    "        return np.random.binomial(1, probabilities)\n",
    "\n",
    "    def train(self, data: np.ndarray, learning_rate: float = 0.1, epochs: int = 10):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(data.shape[0]):\n",
    "                v0 = data[i]\n",
    "                h0 = self.sample_hidden(v0)\n",
    "\n",
    "                v1 = self.sample_visible(h0)\n",
    "                h1 = self.sample_hidden(v1)\n",
    "\n",
    "                positive_gradient = np.outer(v0, h0)\n",
    "                negative_gradient = np.outer(v1, h1)\n",
    "\n",
    "                self.W += learning_rate * (positive_gradient - negative_gradient)\n",
    "                self.b += learning_rate * (v0 - v1)\n",
    "                self.c += learning_rate * (h0 - h1)\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} completed.\")\n",
    "\n",
    "    def transform(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Generate hidden representations for input data.\"\"\"\n",
    "        return sigmoid(np.dot(data, self.W) + self.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed.\n",
      "Epoch 2/5 completed.\n",
      "Epoch 3/5 completed.\n",
      "Epoch 4/5 completed.\n",
      "Epoch 5/5 completed.\n",
      "Classification Accuracy using RBM Features: 0.9255\n"
     ]
    }
   ],
   "source": [
    "# Load and Preprocess MNIST Dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Normalize the data between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Secure Boltzmann Machine\n",
    "n_visible = X_train.shape[1]\n",
    "n_hidden = 128\n",
    "secure_rbm = SecureRBM(n_visible=n_visible, n_hidden=n_hidden)\n",
    "secure_rbm.train(X_train, learning_rate=0.1, epochs=5)\n",
    "\n",
    "# Generate Features using RBM\n",
    "train_features = secure_rbm.transform(X_train)\n",
    "test_features = secure_rbm.transform(X_test)\n",
    "\n",
    "# Classification Task\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(train_features, y_train)\n",
    "predictions = clf.predict(test_features)\n",
    "\n",
    "# Evaluate the Model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Classification Accuracy using RBM Features: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our model could learn with 0.9255 accuracy and we get same accuracy as in the last part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the part 2 of the simulation question. In this part we have implemented the Restricted Boltzmann Machine (RBM) with constrastive divergence (CD) algorithm which perform a Gibbs sampling to approximate the log-likelihood gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecureRBM:\n",
    "    def __init__(self, n_visible: int, n_hidden: int, n_classes: int):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(n_visible, n_hidden) * 0.01\n",
    "        self.U = np.random.randn(n_classes, n_hidden) * 0.01  # Class-hidden weights\n",
    "        self.b = np.zeros(n_visible)  # Visible biases\n",
    "        self.c = np.zeros(n_hidden)   # Hidden biases\n",
    "        self.d = np.zeros(n_classes)  # Class biases\n",
    "\n",
    "    def sample_hidden(self, visible: np.ndarray, label: np.ndarray = None) -> np.ndarray:\n",
    "        \"\"\"Sample hidden units given visible units and labels\"\"\"\n",
    "        activation = np.dot(visible, self.W)\n",
    "        if label is not None:\n",
    "            activation += np.dot(label, self.U)\n",
    "        activation += self.c\n",
    "        probabilities = self.sigmoid(activation)\n",
    "        return np.random.binomial(1, probabilities)\n",
    "\n",
    "    def sample_visible(self, hidden: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sample visible units given hidden units\"\"\"\n",
    "        activation = np.dot(hidden, self.W.T) + self.b\n",
    "        probabilities = self.sigmoid(activation)\n",
    "        return np.random.binomial(1, probabilities)\n",
    "\n",
    "    def sample_label(self, hidden: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Sample label given hidden units\"\"\"\n",
    "        activation = np.dot(hidden, self.U.T) + self.d\n",
    "        probabilities = self.softmax(activation)\n",
    "        return np.random.multinomial(1, probabilities)\n",
    "\n",
    "    def contrastive_divergence(self, v0: np.ndarray, y0: np.ndarray, k: int = 1) -> tuple:\n",
    "        \"\"\"Perform Contrastive Divergence with labels\"\"\"\n",
    "        h0 = self.sample_hidden(v0, y0)\n",
    "\n",
    "        vk = v0.copy()\n",
    "        yk = y0.copy()\n",
    "        \n",
    "        for _ in range(k):\n",
    "            hk = self.sample_hidden(vk, yk)\n",
    "            vk = self.sample_visible(hk)\n",
    "            yk = self.sample_label(hk)\n",
    "\n",
    "        hk = self.sample_hidden(vk, yk)\n",
    "\n",
    "        # Compute gradients\n",
    "        dW = np.outer(v0, h0) - np.outer(vk, hk)\n",
    "        dU = np.outer(y0, h0) - np.outer(yk, hk)\n",
    "        db = v0 - vk\n",
    "        dc = h0 - hk\n",
    "        dd = y0 - yk\n",
    "\n",
    "        return dW, dU, db, dc, dd\n",
    "\n",
    "    def train(self, data: np.ndarray, labels: np.ndarray, learning_rate: float = 0.1, \n",
    "              epochs: int = 10, k: int = 1):\n",
    "        \"\"\"Train the RBM with labeled data\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(data.shape[0]):\n",
    "                v0 = data[i]\n",
    "                y0 = labels[i]\n",
    "                dW, dU, db, dc, dd = self.contrastive_divergence(v0, y0, k)\n",
    "\n",
    "                # Update parameters\n",
    "                self.W += learning_rate * dW\n",
    "                self.U += learning_rate * dU\n",
    "                self.b += learning_rate * db\n",
    "                self.c += learning_rate * dc\n",
    "                self.d += learning_rate * dd\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} completed.\")\n",
    "\n",
    "    def compute_joint_probability(self, visible: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute p(x,y) for all possible classes\"\"\"\n",
    "        hidden_probs = self.sigmoid(np.dot(visible, self.W) + self.c)\n",
    "        class_probs = np.zeros(self.n_classes)\n",
    "        \n",
    "        for y in range(self.n_classes):\n",
    "            y_one_hot = np.zeros(self.n_classes)\n",
    "            y_one_hot[y] = 1\n",
    "            class_activation = np.dot(y_one_hot, self.U)\n",
    "            class_probs[y] = np.sum(hidden_probs * self.sigmoid(class_activation))\n",
    "        \n",
    "        return class_probs\n",
    "\n",
    "    def classify(self, visible: np.ndarray) -> int:\n",
    "        \"\"\"Classify a visible input using p(y|x)\"\"\"\n",
    "        joint_probs = self.compute_joint_probability(visible)\n",
    "        return np.argmax(joint_probs)\n",
    "\n",
    "    def softmax(self,x):\n",
    "        \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training part where i take the data and try to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed.\n",
      "Epoch 2/5 completed.\n",
      "Epoch 3/5 completed.\n",
      "Epoch 4/5 completed.\n",
      "Epoch 5/5 completed.\n",
      "\n",
      "Accuracy: 0.9218\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1343\n",
      "           1       0.97      0.97      0.97      1600\n",
      "           2       0.92      0.91      0.92      1380\n",
      "           3       0.90      0.91      0.90      1433\n",
      "           4       0.94      0.89      0.91      1295\n",
      "           5       0.94      0.86      0.90      1273\n",
      "           6       0.91      0.97      0.94      1396\n",
      "           7       0.91      0.95      0.93      1503\n",
      "           8       0.92      0.87      0.90      1357\n",
      "           9       0.87      0.91      0.89      1420\n",
      "\n",
      "    accuracy                           0.92     14000\n",
      "   macro avg       0.92      0.92      0.92     14000\n",
      "weighted avg       0.92      0.92      0.92     14000\n",
      "\n",
      "\n",
      "Sample Predictions:\n",
      "True: 8, Predicted: 8\n",
      "True: 4, Predicted: 4\n",
      "True: 8, Predicted: 8\n",
      "True: 7, Predicted: 7\n",
      "True: 7, Predicted: 7\n",
      "True: 0, Predicted: 0\n",
      "True: 6, Predicted: 6\n",
      "True: 2, Predicted: 2\n",
      "True: 7, Predicted: 7\n",
      "True: 4, Predicted: 4\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Normalize the data between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Binarize the data\n",
    "binarizer = Binarizer(threshold=0.5)\n",
    "X_binarized = binarizer.fit_transform(X)\n",
    "\n",
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "onehot = OneHotEncoder(sparse_output=False)\n",
    "y_train_onehot = onehot.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "# Initialize RBM\n",
    "n_visible = X_train.shape[1]  # 784 features\n",
    "n_hidden = 256  # Number of hidden units\n",
    "n_classes = 10   # Number of classes (digits 0-9)\n",
    "\n",
    "# Create and train the RBM\n",
    "rbm = SecureRBM(n_visible=n_visible, n_hidden=n_hidden, n_classes=n_classes)\n",
    "rbm.train(\n",
    "    data=X_train,\n",
    "    labels=y_train_onehot,\n",
    "    learning_rate=0.01,\n",
    "    epochs=5,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = []\n",
    "for sample in X_test:\n",
    "    pred = rbm.classify(sample)\n",
    "    predictions.append(pred)\n",
    "\n",
    "# Convert predictions to numpy array\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Calculate and print metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the part 3 of the simulation question. In this part we have implemented the Discriminative Restricted Boltzmann Machine (DiscriminativeRBM) which is a supervised learning algorithm which has a free energy function that is used to calculate the gradient of the weights we used it base on our phase 1 code and we defined a method called train and evaluate which train our model then it will find its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminativeRBM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_categories):\n",
    "        super(DiscriminativeRBM, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)\n",
    "        self.bias_h = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.bias_y = nn.Parameter(torch.zeros(num_categories))\n",
    "        self.weights_output = nn.Parameter(torch.randn(hidden_size, num_categories) * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_probs = torch.sigmoid(torch.matmul(x, self.weights) + self.bias_h)\n",
    "        output_logits = torch.matmul(hidden_probs, self.weights_output) + self.bias_y\n",
    "        return output_logits\n",
    "\n",
    "# Train the Discriminative RBM model\n",
    "def train(model, training_loader, testing_loader, num_epochs=10, learning_rate=0.01):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in training_loader:\n",
    "            inputs, labels = inputs.view(inputs.size(0), -1), labels\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(inputs)\n",
    "            loss = loss_function(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(training_loader):.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Evaluate the Discriminative RBM model\n",
    "def evaluate_accuracy(model, testing_loader):\n",
    "    model.eval()\n",
    "    all_predictions, all_true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in testing_loader:\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "            logits = model(inputs)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_true_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "\n",
    "class MNISTBinaryDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "def load_mnist_binary():\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    binarizer = Binarizer(threshold=0.5)\n",
    "    X_binarized = binarizer.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    y_train = y_train.to_numpy() if isinstance(y_train, pd.Series) else y_train\n",
    "    y_test = y_test.to_numpy() if isinstance(y_test, pd.Series) else y_test\n",
    "    \n",
    "    train_dataset = MNISTBinaryDataset(X_train, y_train)\n",
    "    test_dataset = MNISTBinaryDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.2700\n",
      "Epoch 2/10, Loss: 0.1062\n",
      "Epoch 3/10, Loss: 0.0762\n",
      "Epoch 4/10, Loss: 0.0576\n",
      "Epoch 5/10, Loss: 0.0470\n",
      "Epoch 6/10, Loss: 0.0402\n",
      "Epoch 7/10, Loss: 0.0344\n",
      "Epoch 8/10, Loss: 0.0339\n",
      "Epoch 9/10, Loss: 0.0352\n",
      "Epoch 10/10, Loss: 0.0270\n",
      "Test Accuracy: 0.9658\n",
      "\n",
      "Accuracy: 0.9658\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = load_mnist_binary()\n",
    "\n",
    "# Initialize and train Discriminative RBM\n",
    "discriminative_rbm_model = DiscriminativeRBM(784, 128, 10)\n",
    "trained_model = train(discriminative_rbm_model, train_dataset, test_dataset, num_epochs=10, learning_rate=0.01)\n",
    "\n",
    "# Evaluate the trained model\n",
    "evaluate_accuracy(trained_model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see by using free energy and DiscriminativeRBM we could get higher accuracy around 96.58%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Part 4\n",
    "here we try to implement hybrid learning algorithm which is a combination of unsupervised and supervised learning. We use the unsupervised learning to pretrain the model and then we use the supervised learning to fine tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class HybridDRBM:\n",
    "    def __init__(self, n_visible, n_hidden, n_classes, alpha=0.5):\n",
    "        # Xavier initialization for weights\n",
    "        self.W = np.random.normal(0, np.sqrt(2.0/n_visible), (n_visible, n_hidden))\n",
    "        self.U = np.random.normal(0, np.sqrt(2.0/n_classes), (n_classes, n_hidden))\n",
    "        self.d = np.zeros(n_hidden)\n",
    "        self.c = np.zeros(n_classes)\n",
    "\n",
    "        # Momentum buffers for updates\n",
    "        self.vW = np.zeros_like(self.W)\n",
    "        self.vU = np.zeros_like(self.U)\n",
    "        self.vd = np.zeros_like(self.d)\n",
    "        self.vc = np.zeros_like(self.c)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.alpha = alpha  # Hybrid learning coefficient\n",
    "\n",
    "        # Clip weights to prevent divergence\n",
    "        self.W = np.clip(self.W, -1, 1)\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def _stable_softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / np.sum(e_x)\n",
    "\n",
    "    def free_energy(self, v, y):\n",
    "        wx = np.dot(v, self.W)\n",
    "        uy = np.dot(y, self.U)\n",
    "        hidden = self._relu(self.d + wx + uy)\n",
    "        return -np.sum(hidden) - np.dot(y, self.c)\n",
    "\n",
    "    def train(self, X, y, learning_rate=0.001, momentum=0.9, batch_size=32, epochs=10):\n",
    "        n_samples = len(X)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X, y = X[indices], y[indices]\n",
    "\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_X = X[i:i + batch_size]\n",
    "                batch_y = y[i:i + batch_size]\n",
    "\n",
    "                # Forward pass with ReLU\n",
    "                h_input = self.d + np.dot(batch_X, self.W) + np.dot(batch_y, self.U)\n",
    "                h_act = self._relu(h_input)\n",
    "\n",
    "                # Compute gradients for supervised classification\n",
    "                dW_class = np.dot(batch_X.T, h_act) / batch_size\n",
    "                dU_class = np.dot(batch_y.T, h_act) / batch_size\n",
    "                dd_class = np.mean(h_act, axis=0)\n",
    "                dc_class = np.mean(batch_y, axis=0)\n",
    "\n",
    "                # Compute gradients for unsupervised energy minimization\n",
    "                dW_unsup = np.dot(batch_X.T, h_act) / batch_size\n",
    "                dU_unsup = np.dot(batch_y.T, h_act) / batch_size\n",
    "                dd_unsup = np.mean(h_act, axis=0)\n",
    "                dc_unsup = np.mean(batch_y, axis=0)\n",
    "\n",
    "                # Hybrid update: blend supervised and unsupervised gradients with alpha\n",
    "                dW = self.alpha * dW_class + (1 - self.alpha) * dW_unsup\n",
    "                dU = self.alpha * dU_class + (1 - self.alpha) * dU_unsup\n",
    "                dd = self.alpha * dd_class + (1 - self.alpha) * dd_unsup\n",
    "                dc = self.alpha * dc_class + (1 - self.alpha) * dc_unsup\n",
    "\n",
    "                # Update with momentum\n",
    "                self.vW = momentum * self.vW + learning_rate * dW\n",
    "                self.vU = momentum * self.vU + learning_rate * dU\n",
    "                self.vd = momentum * self.vd + learning_rate * dd\n",
    "                self.vc = momentum * self.vc + learning_rate * dc\n",
    "\n",
    "                self.W += self.vW\n",
    "                self.U += self.vU\n",
    "                self.d += self.vd\n",
    "                self.c += self.vc\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs} completed\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_classes = self.U.shape[0]\n",
    "        predictions = []\n",
    "        \n",
    "        for v in X:\n",
    "            # Compute free energy for each class\n",
    "            energies = np.array([\n",
    "                -self.free_energy(v.reshape(1, -1), np.eye(n_classes)[c].reshape(1, -1))\n",
    "                for c in range(n_classes)\n",
    "            ]).flatten()  # Ensure a flat list of energy values\n",
    "            \n",
    "            prob = self._stable_softmax(energies)\n",
    "            predictions.append(np.argmax(prob))\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Hybrid DRBM with alpha=0.01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sk2ip/.local/lib/python3.12/site-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed\n",
      "Epoch 2/5 completed\n",
      "Epoch 3/5 completed\n",
      "Epoch 4/5 completed\n",
      "Epoch 5/5 completed\n",
      "\n",
      "Accuracy for alpha = 0.01: 0.0959\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      1.00      0.18      1343\n",
      "           1       1.00      0.00      0.00      1600\n",
      "           2       1.00      0.00      0.00      1380\n",
      "           3       1.00      0.00      0.00      1433\n",
      "           4       1.00      0.00      0.00      1295\n",
      "           5       1.00      0.00      0.00      1273\n",
      "           6       1.00      0.00      0.00      1396\n",
      "           7       1.00      0.00      0.00      1503\n",
      "           8       1.00      0.00      0.00      1357\n",
      "           9       1.00      0.00      0.00      1420\n",
      "\n",
      "    accuracy                           0.10     14000\n",
      "   macro avg       0.91      0.10      0.02     14000\n",
      "weighted avg       0.91      0.10      0.02     14000\n",
      "\n",
      "Training Hybrid DRBM with alpha=0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sk2ip/.local/lib/python3.12/site-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed\n",
      "Epoch 2/5 completed\n",
      "Epoch 3/5 completed\n",
      "Epoch 4/5 completed\n",
      "Epoch 5/5 completed\n",
      "\n",
      "Accuracy for alpha = 0.1: 0.0959\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      1.00      0.18      1343\n",
      "           1       1.00      0.00      0.00      1600\n",
      "           2       1.00      0.00      0.00      1380\n",
      "           3       1.00      0.00      0.00      1433\n",
      "           4       1.00      0.00      0.00      1295\n",
      "           5       1.00      0.00      0.00      1273\n",
      "           6       1.00      0.00      0.00      1396\n",
      "           7       1.00      0.00      0.00      1503\n",
      "           8       1.00      0.00      0.00      1357\n",
      "           9       1.00      0.00      0.00      1420\n",
      "\n",
      "    accuracy                           0.10     14000\n",
      "   macro avg       0.91      0.10      0.02     14000\n",
      "weighted avg       0.91      0.10      0.02     14000\n",
      "\n",
      "Training Hybrid DRBM with alpha=1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sk2ip/.local/lib/python3.12/site-packages/numpy/core/_methods.py:118: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/tmp/ipykernel_5453/4173820447.py:64: RuntimeWarning: invalid value encountered in multiply\n",
      "  dW = self.alpha * dW_class + (1 - self.alpha) * dW_unsup\n",
      "/tmp/ipykernel_5453/4173820447.py:66: RuntimeWarning: invalid value encountered in multiply\n",
      "  dd = self.alpha * dd_class + (1 - self.alpha) * dd_unsup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed\n",
      "Epoch 2/5 completed\n",
      "Epoch 3/5 completed\n",
      "Epoch 4/5 completed\n",
      "Epoch 5/5 completed\n",
      "\n",
      "Accuracy for alpha = 1.0: 0.0959\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      1.00      0.18      1343\n",
      "           1       1.00      0.00      0.00      1600\n",
      "           2       1.00      0.00      0.00      1380\n",
      "           3       1.00      0.00      0.00      1433\n",
      "           4       1.00      0.00      0.00      1295\n",
      "           5       1.00      0.00      0.00      1273\n",
      "           6       1.00      0.00      0.00      1396\n",
      "           7       1.00      0.00      0.00      1503\n",
      "           8       1.00      0.00      0.00      1357\n",
      "           9       1.00      0.00      0.00      1420\n",
      "\n",
      "    accuracy                           0.10     14000\n",
      "   macro avg       0.91      0.10      0.02     14000\n",
      "weighted avg       0.91      0.10      0.02     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Learning \n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "binarizer = Binarizer(threshold=0.5)\n",
    "X_binarized = binarizer.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.2, random_state=42)\n",
    "onehot = OneHotEncoder(sparse_output=False)\n",
    "y_train_onehot = onehot.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "alphas = [0.01, 0.1, 1.0]\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = HybridDRBM(n_visible=784, n_hidden=256, n_classes=10, alpha=alpha)\n",
    "    print(f\"Training Hybrid DRBM with alpha={alpha}...\")\n",
    "    model.train(X_train, y_train_onehot, learning_rate=0.01, batch_size=32, epochs=5)\n",
    "    \n",
    "    # Evaluate\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"\\nAccuracy for alpha = {alpha}: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, predictions, zero_division=1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our model could learn by adding hybrid learning and we could not get more accuracy by changing epochs and learning rate or adding momentum and this was our results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
